{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Advanced Polars Analytics with pyhdb-rs\n",
    "\n",
    "This notebook demonstrates how to leverage the **zero-copy Arrow integration**\n",
    "between SAP HANA and Polars for high-performance analytics.\n",
    "\n",
    "## Why Zero-Copy Matters\n",
    "\n",
    "Traditional workflow:\n",
    "```\n",
    "HANA → Network → Python objects → pandas/numpy → Analysis\n",
    "       ↑ slow        ↑ GC pressure      ↑ memory copy\n",
    "```\n",
    "\n",
    "pyhdb-rs workflow:\n",
    "```\n",
    "HANA → Network → Arrow buffers → Polars\n",
    "                 ↑ zero-copy, no Python objects!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyhdb_rs import connect\n",
    "import polars as pl\n",
    "\n",
    "HANA_URL = os.environ.get(\"HANA_TEST_URI\")\n",
    "\n",
    "# Polars configuration for large datasets\n",
    "pl.Config.set_tbl_rows(20)\n",
    "pl.Config.set_fmt_str_lengths(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lazy-section",
   "metadata": {},
   "source": [
    "## LazyFrame for Deferred Execution\n",
    "\n",
    "Polars LazyFrames allow query optimization before execution.\n",
    "Combined with HANA's query pushdown, you get optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lazy-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sales_data() -> pl.LazyFrame:\n",
    "    \"\"\"Load sales data as LazyFrame for deferred processing.\"\"\"\n",
    "    with connect(HANA_URL) as conn:\n",
    "        with conn.cursor() as cursor:\n",
    "            # Push filtering to HANA - only transfer what we need\n",
    "            df = cursor.execute_polars(\"\"\"\n",
    "                SELECT \n",
    "                    SALE_ID,\n",
    "                    SALE_DATE,\n",
    "                    CUSTOMER_ID,\n",
    "                    PRODUCT_ID,\n",
    "                    QUANTITY,\n",
    "                    UNIT_PRICE,\n",
    "                    DISCOUNT,\n",
    "                    REGION\n",
    "                FROM SALES\n",
    "                WHERE SALE_DATE >= '2024-01-01'\n",
    "            \"\"\")\n",
    "            return df.lazy()\n",
    "\n",
    "# Create LazyFrame - no computation yet!\n",
    "sales_lf = load_sales_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lazy-transform",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations (still lazy - no execution)\n",
    "result = (\n",
    "    sales_lf\n",
    "    .with_columns([\n",
    "        # Calculate total amount\n",
    "        (pl.col(\"QUANTITY\") * pl.col(\"UNIT_PRICE\") * (1 - pl.col(\"DISCOUNT\"))).alias(\"TOTAL\"),\n",
    "        # Extract month\n",
    "        pl.col(\"SALE_DATE\").dt.month().alias(\"MONTH\"),\n",
    "    ])\n",
    "    .filter(pl.col(\"TOTAL\") > 100)  # Filter calculated column\n",
    "    .group_by([\"REGION\", \"MONTH\"])\n",
    "    .agg([\n",
    "        pl.col(\"TOTAL\").sum().alias(\"REVENUE\"),\n",
    "        pl.col(\"SALE_ID\").count().alias(\"ORDER_COUNT\"),\n",
    "        pl.col(\"TOTAL\").mean().alias(\"AVG_ORDER_VALUE\"),\n",
    "    ])\n",
    "    .sort([\"REGION\", \"MONTH\"])\n",
    ")\n",
    "\n",
    "# Now execute and collect results\n",
    "monthly_revenue = result.collect()\n",
    "print(monthly_revenue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "window-section",
   "metadata": {},
   "source": [
    "## Window Functions\n",
    "\n",
    "Polars provides powerful window functions for running totals, rankings, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "window-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect(HANA_URL) as conn:\n",
    "    with conn.cursor() as cursor:\n",
    "        df = cursor.execute_polars(\"\"\"\n",
    "            SELECT \n",
    "                EMPLOYEE_ID,\n",
    "                DEPARTMENT,\n",
    "                SALE_DATE,\n",
    "                AMOUNT\n",
    "            FROM EMPLOYEE_SALES\n",
    "            WHERE SALE_DATE >= '2024-01-01'\n",
    "        \"\"\")\n",
    "\n",
    "# Window functions in Polars\n",
    "result = df.with_columns([\n",
    "    # Running total per employee\n",
    "    pl.col(\"AMOUNT\")\n",
    "        .cum_sum()\n",
    "        .over(\"EMPLOYEE_ID\")\n",
    "        .alias(\"RUNNING_TOTAL\"),\n",
    "    \n",
    "    # Rank within department\n",
    "    pl.col(\"AMOUNT\")\n",
    "        .rank(descending=True)\n",
    "        .over(\"DEPARTMENT\")\n",
    "        .alias(\"DEPT_RANK\"),\n",
    "    \n",
    "    # Percentage of department total\n",
    "    (pl.col(\"AMOUNT\") / pl.col(\"AMOUNT\").sum().over(\"DEPARTMENT\") * 100)\n",
    "        .round(2)\n",
    "        .alias(\"DEPT_PERCENTAGE\"),\n",
    "    \n",
    "    # Moving average (7-day)\n",
    "    pl.col(\"AMOUNT\")\n",
    "        .rolling_mean(window_size=7)\n",
    "        .over(\"EMPLOYEE_ID\")\n",
    "        .alias(\"MA_7D\"),\n",
    "])\n",
    "\n",
    "print(result.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "join-section",
   "metadata": {},
   "source": [
    "## Efficient Joins\n",
    "\n",
    "Load dimension tables once, join in Polars for repeated analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "join-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect(HANA_URL) as conn:\n",
    "    with conn.cursor() as cursor:\n",
    "        # Load fact table (large)\n",
    "        orders = cursor.execute_polars(\"\"\"\n",
    "            SELECT ORDER_ID, CUSTOMER_ID, PRODUCT_ID, QUANTITY, ORDER_DATE\n",
    "            FROM ORDERS \n",
    "            WHERE ORDER_DATE >= '2024-01-01'\n",
    "        \"\"\")\n",
    "        \n",
    "        # Load dimension tables (small, can cache)\n",
    "        customers = cursor.execute_polars(\"\"\"\n",
    "            SELECT CUSTOMER_ID, CUSTOMER_NAME, SEGMENT, COUNTRY\n",
    "            FROM CUSTOMERS\n",
    "        \"\"\")\n",
    "        \n",
    "        products = cursor.execute_polars(\"\"\"\n",
    "            SELECT PRODUCT_ID, PRODUCT_NAME, CATEGORY, UNIT_PRICE\n",
    "            FROM PRODUCTS\n",
    "        \"\"\")\n",
    "\n",
    "print(f\"Orders: {len(orders):,} rows\")\n",
    "print(f\"Customers: {len(customers):,} rows\")\n",
    "print(f\"Products: {len(products):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "join-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join and analyze in Polars (very fast!)\n",
    "enriched = (\n",
    "    orders\n",
    "    .join(customers, on=\"CUSTOMER_ID\", how=\"left\")\n",
    "    .join(products, on=\"PRODUCT_ID\", how=\"left\")\n",
    "    .with_columns(\n",
    "        (pl.col(\"QUANTITY\") * pl.col(\"UNIT_PRICE\")).alias(\"TOTAL\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Segment analysis\n",
    "segment_analysis = (\n",
    "    enriched\n",
    "    .group_by([\"SEGMENT\", \"CATEGORY\"])\n",
    "    .agg([\n",
    "        pl.col(\"TOTAL\").sum().alias(\"REVENUE\"),\n",
    "        pl.col(\"ORDER_ID\").n_unique().alias(\"ORDERS\"),\n",
    "        pl.col(\"CUSTOMER_ID\").n_unique().alias(\"CUSTOMERS\"),\n",
    "    ])\n",
    "    .with_columns(\n",
    "        (pl.col(\"REVENUE\") / pl.col(\"ORDERS\")).round(2).alias(\"AVG_ORDER_VALUE\")\n",
    "    )\n",
    "    .sort(\"REVENUE\", descending=True)\n",
    ")\n",
    "\n",
    "print(segment_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pivot-section",
   "metadata": {},
   "source": [
    "## Pivot Tables\n",
    "\n",
    "Create pivot tables for cross-tabulation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pivot-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly revenue by region (pivot)\n",
    "monthly_pivot = (\n",
    "    enriched\n",
    "    .with_columns(pl.col(\"ORDER_DATE\").dt.strftime(\"%Y-%m\").alias(\"MONTH\"))\n",
    "    .group_by([\"COUNTRY\", \"MONTH\"])\n",
    "    .agg(pl.col(\"TOTAL\").sum().alias(\"REVENUE\"))\n",
    "    .pivot(\n",
    "        on=\"MONTH\",\n",
    "        index=\"COUNTRY\",\n",
    "        values=\"REVENUE\",\n",
    "    )\n",
    "    .fill_null(0)\n",
    ")\n",
    "\n",
    "print(monthly_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timeseries-section",
   "metadata": {},
   "source": [
    "## Time Series Analysis\n",
    "\n",
    "Polars has excellent support for time series operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timeseries-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect(HANA_URL) as conn:\n",
    "    with conn.cursor() as cursor:\n",
    "        # Load time series data\n",
    "        metrics = cursor.execute_polars(\"\"\"\n",
    "            SELECT \n",
    "                TIMESTAMP,\n",
    "                SENSOR_ID,\n",
    "                TEMPERATURE,\n",
    "                HUMIDITY,\n",
    "                PRESSURE\n",
    "            FROM IOT_METRICS\n",
    "            WHERE TIMESTAMP >= ADD_DAYS(CURRENT_TIMESTAMP, -30)\n",
    "            ORDER BY TIMESTAMP\n",
    "        \"\"\")\n",
    "\n",
    "# Resample to hourly aggregates\n",
    "hourly = (\n",
    "    metrics\n",
    "    .sort(\"TIMESTAMP\")\n",
    "    .group_by_dynamic(\n",
    "        \"TIMESTAMP\",\n",
    "        every=\"1h\",\n",
    "        group_by=\"SENSOR_ID\",\n",
    "    )\n",
    "    .agg([\n",
    "        pl.col(\"TEMPERATURE\").mean().alias(\"AVG_TEMP\"),\n",
    "        pl.col(\"TEMPERATURE\").min().alias(\"MIN_TEMP\"),\n",
    "        pl.col(\"TEMPERATURE\").max().alias(\"MAX_TEMP\"),\n",
    "        pl.col(\"HUMIDITY\").mean().alias(\"AVG_HUMIDITY\"),\n",
    "        pl.len().alias(\"READINGS\"),\n",
    "    ])\n",
    ")\n",
    "\n",
    "print(f\"Resampled to {len(hourly):,} hourly records\")\n",
    "print(hourly.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anomaly-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect anomalies using rolling statistics\n",
    "anomalies = (\n",
    "    hourly\n",
    "    .sort([\"SENSOR_ID\", \"TIMESTAMP\"])\n",
    "    .with_columns([\n",
    "        # Rolling mean and std\n",
    "        pl.col(\"AVG_TEMP\")\n",
    "            .rolling_mean(window_size=24)\n",
    "            .over(\"SENSOR_ID\")\n",
    "            .alias(\"ROLLING_MEAN\"),\n",
    "        pl.col(\"AVG_TEMP\")\n",
    "            .rolling_std(window_size=24)\n",
    "            .over(\"SENSOR_ID\")\n",
    "            .alias(\"ROLLING_STD\"),\n",
    "    ])\n",
    "    .with_columns(\n",
    "        # Z-score for anomaly detection\n",
    "        ((pl.col(\"AVG_TEMP\") - pl.col(\"ROLLING_MEAN\")) / pl.col(\"ROLLING_STD\"))\n",
    "            .abs()\n",
    "            .alias(\"Z_SCORE\")\n",
    "    )\n",
    "    .filter(pl.col(\"Z_SCORE\") > 3)  # More than 3 standard deviations\n",
    ")\n",
    "\n",
    "print(f\"Detected {len(anomalies)} anomalies\")\n",
    "print(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-section",
   "metadata": {},
   "source": [
    "## Export Results\n",
    "\n",
    "Polars supports various output formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to Parquet (columnar, compressed)\n",
    "segment_analysis.write_parquet(\"segment_analysis.parquet\")\n",
    "\n",
    "# Export to CSV\n",
    "segment_analysis.write_csv(\"segment_analysis.csv\")\n",
    "\n",
    "# Export to JSON\n",
    "segment_analysis.write_json(\"segment_analysis.json\")\n",
    "\n",
    "# Convert to pandas for visualization libraries\n",
    "pandas_df = segment_analysis.to_pandas()\n",
    "\n",
    "print(\"Exported to parquet, csv, and json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory-section",
   "metadata": {},
   "source": [
    "## Memory Efficiency\n",
    "\n",
    "Check memory usage of your DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "memory-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_usage_mb(df: pl.DataFrame) -> float:\n",
    "    \"\"\"Calculate DataFrame memory usage in MB.\"\"\"\n",
    "    return df.estimated_size() / 1024 / 1024\n",
    "\n",
    "print(f\"Orders: {memory_usage_mb(orders):.2f} MB\")\n",
    "print(f\"Enriched: {memory_usage_mb(enriched):.2f} MB\")\n",
    "print(f\"Segment Analysis: {memory_usage_mb(segment_analysis):.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
